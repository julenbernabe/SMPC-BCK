#Si que se puede hacer con clases, mirarlo !


#------ARBOL DECISION DE CLASIFICACION----------
#La variable Y es cualitativa. 
#Se podria hacer algo similar siendo Y cuantitativa para construir un arbol de regresion 

#ARBOLES DE REGRESION:
#   - Variable dependiente es continua
#   - Valores de los nodos terminales se reducen a la media de las observaciones en esa region.

#ARBOLES DE CLASIFICACION:
#   - Variable dependiente es categorica
#   - El valor en el nodo terminal se reduce a la moda de las observaciones del conjunto
#   de entrenamiento que han caido en esa region. Es decir, el elemento mas repetido.


#CODIGO DE PYTHON INSPIRADO EN:
#https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb

#Cambios realizados para adaptarlo a SCALE-MAMBA

#En este codigo faltaria el preprocesamiento de los datos donde se 
#se separa el conjunto de datos en un conjunto de entrenemiento y en
#otro de validacion

#Para realizar este ejemplo directamente se dan los datos de 
#entrenamiento y validacion por separado

#Los datos se guardan en una matriz data
#   Las primeras columnas representan cada una de las variables explicativas X1,...,Xn
#   La ultima columna representa la variable respuesta Y

#El arbol de clasificacion consiste en eparar los datos basandose en los valores de las
#variables X1,...,Xn e identificar la variable que crea los conjuntos mas homogeneos y 
#que a su vez son heterogeneos entre ellos (fijandonos en los valors de Y)


#DESVENTAJAS:
#   Sobreajuste (Se puede solucionar metiendo varios arboles de decision en un Random Forest)
#   Perdida de informacion al categorizar variables continuas
#   Precision: metodos como SVM y clasificadores tipo ensamblador a menudo tienen tasas de error 30% mas bajas que CART
#   Inestabilidad: un pequeno cambio en los datos puede modificar ampliamente la estructura del arbol.
#Por lo tanto la interpretacion no es tan directa como parece.

#tipo: {"regresion","clasificacion"}
#criterio : {"gini", "entropy"}. La funcion para medir la calidad de una division.
#Seleccionar que variable elegir para obtener la mejor division. Dos opciones:
#               Calculando la impureza de Gini
#               Entropia: para la ganancia de informacion
#               Este parametro es especifico de los arboles de decisiones


#---------------Ejemplo utilizando la impureza de gini-------------


#CONJUNTO DE ENTRENAMIENTO:

#training_data = [['Green', 3, 'Apple'],
#                 ['Yellow', 3, 'Apple'],
#                 ['Red', 1, 'Grape'],
#                 ['Red', 1, 'Grape'],
#                 ['Yellow', 3, 'Lemon']]

#X1: color (cualitativa)
#X2: diameter (cuantitativa)
#Y: label (cualitativa)

#En SCALE-MAMBA para trabajar con variable cualitativa, como no se puede usar
#string lo guardamos como numeros, pero hay que indicar de alguna forma que en
#realidad es cualitativa. 

#Asignamos los numeros para color:
#   'Green':0
#   'Yellow':1
#   'Red':2

#Asignamos los numero para label:
#   'Apple':0
#   'Grape':1
#   'Lemon':2

#Todo esto habra que hacerlo en el preprocesamiento y recibir directamente 
#del .txt datos numericos, de hecho por el momento solo sabemos recibir sint
#y a lo sumo transformalo a sfix.

#Indicamos mediante una lista si las variables X1,X2 e Y son cuantitvas o cualitativas
#en ese orden.
#   Si es cualitativa, no es numerica:0
#   Si es cuantitativa, es numerica:1

numeric_list=[0,1,0]

#Por lo tanto nuestro data sera:

#training_data = [[0, 3, 0],
#                 [1, 3, 0],
#                 [2, 1, 1],
#                 [2, 1, 1],
#                 [1, 3, 2]]

#Esto pensar como vendra dado del .txt:
training_data=sfix.Matrix(5,3)
'''
training_data=[[0,0,0],
                [0,0,0],
                [0,0,0],
                [0,0,0],
                [0,0,0]]
'''

training_data[0][0]=sfix(0)
training_data[0][1]=sfix(3)
training_data[0][2]=sfix(0)

training_data[1][0]=sfix(1)
training_data[1][1]=sfix(3)
training_data[1][2]=sfix(0)

training_data[2][0]=sfix(2)
training_data[2][1]=sfix(1)
training_data[2][2]=sfix(1)

training_data[3][0]=sfix(2)
training_data[3][1]=sfix(1)
training_data[3][2]=sfix(1)

training_data[4][0]=sfix(1)
training_data[4][1]=sfix(3)
training_data[4][2]=sfix(2)



#------------------FUNCION valores_unicos-----------------------
#Funcion para encontrar los valores unicos de una columna en un dataset-
#Es decir, en el caso de las variables cualitativas nos sirve para saber cuales son las 
#distintas categorias posibles de la variable "col".

#Hacer una lista poniendo un 1 si no ha salido y un 0 si ya esta : valor_repetido
#Sumar los unos
#Crear array de esa longitud y meter en el array los que tengan un uno
#Eso no se puede hacer porque te sale el error de: size must be known at compile time

def valores_repetidos(data,col):
    n=len(data)
    valor_repetido=sfix.Array(n)

    @for_range(n)
    def range_body(i):
        #miramos el elemento de la fila correspondiente a la columan
        valor=data[i][col]

        #Miramos por cada uno de los elementos de la columna hasta ese numero a ver si estaba antes:
        esta=MemValue(sint(0))
        @for_range(i)
        def range_body(j):
            esta.write(esta+(data[j][col] == valor))

        #Si ya esta ponemos un cero, si no esta ponemos un uno
        @if_e((esta==1).reveal()==1)
        def block():
            valor_repetido[i]=sfix(1)
        @else_
        def block():
            valor_repetido[i]=sfix(0)

    return valor_repetido 

         

#-----Comprobar que funciona valores_unicos-----------------
#Una lista de valores unicos como tal es imposible de hacer
#Pero la funcion de valores repetidos si que funciona 

print_ln('\n ELEMENTOS REPETIDOS \n')
for i in range(len(valores_repetidos(training_data,0))):
    elemento=valores_repetidos(training_data,0)[i]
    print_ln('Elemento repetido en la variable color : %s',elemento.reveal())
    
#Somos capaces de conseguir esto:
#La columna de la variable color y el vector indicando si esta repetido o no:
# [0 1 2 2 1]
# [0 0 0 1 1]

#------------------FUNCION noelementos----------------------------
#Para encontrar el numero de elementos que hay de un tipo de label concreto.
#Es decir, de una de las categorias de la variable Y cuantos elementos hay

def noelementos(data,tipo):
    count=sfix.Array(1)
    count[0]=sfix(0)
    for i in range(len(data)):
        row_label=data[i][-1]
        
        #si es del tipo concreto que estamos contando sumamos uno
        @if_e((row_label==tipo).reveal()==1)
        def block():
            count[0]=count[0]+1
        @else_
        def block():
        #si no se queda igual 
            count[0]=count[0]
    return count[0]

#----------Comprobar que funciona noelementos--------------
a=noelementos(training_data,2)
print_ln('no elementos con label Lemon: %s',a.reveal())
b=noelementos(training_data,1)
print_ln('no elementos con label Grape: %s',b.reveal())
c=noelementos(training_data,0)
print_ln('no elementos con label Apple: %s',c.reveal())

#FUNCIONA CORRECTAMENTE


#FALTA FUNCION lista_no_elem NO PUEDE FUNCIONAR SIN QUE FUNCINE valores_unicos


#-------------------FUNCION question ------------------------
#Es una lista con dos atributos:
#   columna: columna de donde se saca la pregunta
#   valor: el valor que toma en dicha columna

#Es decir, para hacer las preguntas:
#   Es color==green? Se hace question(0,0)
#                    colos es la columna 0
#                    green es el valor 0

#   Es diameter >=3? Se hace question (1,3)
#                    diameter es la columna 1

def question(columna,valor):
    return [columna,valor]


#---------------------FUNCION match_question----------------------------
def match_question(question,row,numeric_list):

    #primero comparamos si la pregunta la hacemos sobre una variable categorica o numerica
    #question[0] guarda la columna, es decir, la variable 
    variable=question[0]
    numeric=numeric_list[variable] 
    valor=row[variable]

    result=sfix.Array(1)
    result[0]=sfix(0)

    if_then(numeric==1)
    @if_e((valor >= question[1]).reveal()==1)
    def block():
        result[0]=sfix(1)
    @else_
    def block():
        result[0]=sfix(0)

    else_then()
    @if_e((valor == question[1]).reveal()==1)
    def block():
        result[0]=sfix(1)
    @else_
    def block():
        result[0]=sfix(0)
    end_if()

    return result[0]

#---------------Comprobar que funciona match_question----------------
#Cogemos la pregunta Is color==green
q=question(0,sfix(0))

#   Cogemos la fila 0 como ejemplo, es decir la fila [0, 3, 0]. La repuesta debe ser true
example1=training_data[0]
print_ln('Para la fila cero el color SI  es verde: %s',match_question(q,example1,numeric_list).reveal())

#   Cogemos la fila 2 como ejemlo, es decir la fila [2, 1, 1]. La respuesta deber ser false
example2=training_data[2]
print_ln('Para la fila dos el color NO es verde: %s',match_question(q,example2,numeric_list).reveal())

#Para la pregunta is diameter >=3
q2=question(1,sfix(3))

#   Cogemos la fila 0 como ejemplo, es decir la fila [0, 3, 0]. La repuesta debe ser true
example3=training_data[0]
print_ln('Para la fila cero el diametro SI  es mayor o igual que 3: %s',match_question(q2,example3,numeric_list).reveal())

#   Cogemos la fila 2 como ejemlo, es decir la fila [2, 1, 1]. La respuesta deber ser false
example4=training_data[2]
print_ln('Para la fila dos el diametro NO es mayor o igual que tres : %s',match_question(q2,example4,numeric_list).reveal())

#FUNCIONA CORRECTAMENTE!!!!!!!!!!

#----------------------------FUNCION partition-------------------------
#Se encarga de partir el dataset en dos datasets dependiendo de la respuesta a la pregunta
#Para cada linea de un dataset, comprobar si la respuesta a la pregunta es verdadero o falso 
#   Si es verdadero se agrega la linea a true_rows
#   Si es falsa se agrega la linea a false_rows

#       true_rows_bits: 1 si la linea esta dentro de true_rows

# Dependiendo de la iteration  y el nodo haremos que ponga unos numeros o otros

def partition(data, partition_list,question,numeric_list,iteration,nodo,p):

    partition_nueva=sfix.Array(len(data))

    nodo_false=nodo+p+2**iteration

    nodo_true=nodo+p+1+2**iteration

    

    @for_range(len(data))
    def range_body(i): 
        #Comprobamos si esa fila pertenece al nodo que queremos dividir
        @if_e((partition_list[i]==nodo).reveal())
        def block():
            row=data[i]
       
            @if_e((match_question(question,row,numeric_list)).reveal()==1)
            def block():
                partition_nueva[i]=nodo_true

            @else_
            def block():
                partition_nueva[i]=nodo_false

        #Si no pertenece al nodo que queremos dividir se queda igual
        @else_
        def block():
            partition_nueva[i]=partition_list[i]

 
    return partition_nueva

#---------------Comprobamos que funciona partition--------------

true_rows=partition(training_data,question(0,sfix(2)),numeric_list)

print_ln('Las lineas que pertencen a true (son las que estan en la posicion del indice con valor 1): ')

print_ln('Las lineas que pertencen a false (son las que estan en la posicion del indice con valor 0): ')

for i in range(len(true_rows)):
    print_ln('color: %s',true_rows[i].reveal())



#----------------FUNCION imprimir filas_partition----------

def filas_partition(data,question,numeric_list ):
    n=len(data)
    true_rows_bits=partition(data, question,numeric_list)
    
    @for_range(len(data))
    def range_body(i): 
        @if_e((true_rows_bits[i]).reveal()==1)
        def block():
            print_ln('\n Esta fila pertenece a true row:')
            @for_range(len(data[i]))
            def range_body(j): 
                print_str('%s ', data[i][j].reveal()) #Asi se imprime todo el vector en una linea

        @else_
        def block():
            print_ln('\n Esta fila pertenece a false row:')    
            @for_range(len(data[i]))
            def range_body(j): 
                print_str('%s ', data[i][j].reveal()) #Asi se imprime todo el vector en una linea

#--------------Comprobamos que funciona imprimir filas_partition------------
filas_partition(training_data,question(0,sfix(2)),numeric_list)


#-------------------------------FUNCION gini---------------------------
#Sirve para calcular la impureza de gini
#Como de frecuente es que un elemento elegido de forma random sea clasificado incorrectamente
    #si se elige el tipo de forma random.
    #Es decir, si tienes un bol con naranjas, mandarinas y manzanas
    #y otro bol con papeles en los que pone naranja, mandarina o manzana
    #Se elige de cada bol un elemento de forma random.
    #La frecuencia de que no coincida se mide con la impureza de gini.
#formula sacada de : https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity


#Esto no tiene por que ser secreto y facilita muchos las cosas
valores_y=[0,1,2]
valores_y=sfix.Array(3)
valores_y[0]=sfix(0)
valores_y[1]=sfix(1)
valores_y[2]=sfix(2)

def gini(data,valores_y):
    n=len(data)
    m=len(valores_y) #Numero de categorias de la variable y
    counts=sfix.Array(m) #No es n, es el numero de tipos distinto
    #En counts lo que tenemos que guardar es por cada tipo diferente de ultima variable
    #Esto lo podemos saber de antes, predefinido. No es un valor secretos
    
    @for_range(m)
    def range_body(i): 
        counts[i]=noelementos(data,valores_y[i])
    
    #La formula es:
    #I=1- sum(i=1,2,...,J)p_i^2
    #suponiendo que J es el numero de clases y p_i la fraccion de items clasificados como clase i
    
    impurity=sfix.Array(1)
    impurity[0]=sfix(1)

    for i in range(len(counts)):
        p_i = counts[i] / sfix(n)
        impurity[0] =impurity[0] - p_i*p_i

    return impurity[0]

#------------------Comprobamos que funciona gini-----------------
#Ejemplo 0
print_ln('gini del training data: %s', gini(training_data,valores_y).reveal())

#Ejemplo 1
no_mixing=sfix.Matrix(2,1)
no_mixing[0][0]=sfix(0)
no_mixing[1][0]=sfix(0)
print_ln('gini del no_mixing: %s', gini(no_mixing,valores_y).reveal())

#Ejemplo 2
some_mixing=sfix.Matrix(2,1)
some_mixing[0][0]=sfix(0)
some_mixing[1][0]=sfix(1)
print_ln('gini del some_mixing: %s', gini(some_mixing,valores_y).reveal())

#Ejemplo 3
lots_of_mixing=sfix.Matrix(5,1)
lots_of_mixing[0][0]=sfix(0)
lots_of_mixing[1][0]=sfix(1)
lots_of_mixing[2][0]=sfix(2)
lots_of_mixing[3][0]=sfix(3)
lots_of_mixing[4][0]=sfix(4)
print_ln('gini del lots_of_mixing: %s', gini(lots_of_mixing,valores_y).reveal())

#-------------------------FUNCION gini_2()-------------------
#En realidad, no tendremos una lista con el data dividido, por lo que hay que modificar la funcion de gini
#para que dentro divida el data
def gini_2(data,valores_y,partition_list,nodo):
    
    m=len(valores_y) #Numero de categorias de la variable y

    counts=sfix.Array(m) 
    #En counts lo que tenemos que guardar es por cada tipo diferente de ultima variable
    #Esto lo podemos saber de antes, predefinido. No es un valor secretos
    
    #Guadamos en una variable el numero de elementos de ese nodo:
    q=sfix.Array(1)
    q[0]=sfix(0)

    @for_range(m)
    def range_body(i): 
        categoria=valores_y[i]

        #No queremos mirar entre todos los valores de data, solo los que pertenecen a ese nodo concreto
        @for_range(len(data))
        def range_body(j):
            #Miramos los que en la partition_list tengan justo ese numero de nodo concreto
            @if_e((partition_list[j]==nodo).reveal())
            def block():
                #Sumamos uno al numero de elementos de ese nodo:
                q[0]=q[0]+1

                #Es el nodo del que queremos calcular gini, ahora comprobamos si el valor 
                #de data correspondiente a ese indice es igual que la categoria
                @if_e((data[j]==categoria).reveal())
                def block():
                    counts[i]=counts[i]+1
                @else_
                def block():
                    counts[i]=counts[i]
            @else_
            def block():
                counts[i]=counts[i]
        
    
    #La formula es:
    #I=1- sum(i=1,2,...,m)p_i^2
    #suponiendo que m es el numero de clases y p_i la fraccion de items clasificados como clase i
    
    impurity=sfix.Array(1)
    impurity[0]=sfix(1)

    @for_range(m)
    def range_body(i):
        p_i = counts[i] / q[0]
        impurity[0] =impurity[0] - p_i*p_i

    return impurity[0]



#-----------------------FUNCION info_gain------------------------
# La uncertainity del nodo de entrada(es decir el indice de gini) menos las impurezas ponderaas de los dos hijos
# En este caso nodo es el nodo anterior a relizar la partition list !!!!!!
# Es decir partition list es la de los nodos hijos, peor nodo es el nodo madre, que ya no estara en partition_list
#COMPROBAR QUE LO DE LEFT Y RIGHT NO ESTE TODO AL REVES !!!!

def info_gain(data,valores_y,partition_list, current_uncertainty,nodo,iteration,p):

    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE FALSE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
    es_false=nodo+p+2**iteration

    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE TRUE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
    es_true=nodo+p+1+2**iteration

    #len(left) es el numero de trues(???), es decir, el numero de es_true
    trues=MemFix(sfix(0))

    #len(right) es el numero de falses(???), es decir, el numero de es_false
    falses=MemFix(sfix(0))

    @for_range(len(partition_list))
    def range_body(i): 
        @if_e((partition_list[i]).reveal()==es_true)
        def block():
            trues.iadd(1)
        @else_
        def block():
            trues.iadd(0)
        
        @if_e((partition_list[i]).reveal()==es_false)
        def block():
            falses.iadd(1)
        @else_
        def block():
            falses.iadd(0)

    
    #Para calcular gini(left) y gini(right) necesitamos una lista con los left y los right o
    #cambiar la funcion gini. Voy a utilizar gini_2
  
    p = trues / (trues+falses)
    return current_uncertainty - p * gini_2(data,valores_y,partition_list,es_true) - (1 - p) * gini_2(data,valores_y,partition_list,es_false)

#---------------Comprobamos que funciona info_gain----------------
current_uncertainty = gini(training_data,valores_y)
print_ln('la current_uncertainty: %s', current_uncertainty.reveal())

true_rows_bits = partition(training_data, question(0, 0),numeric_list)
print_ln('La ganancia de informacion al partirlo por green: %s',info_gain(true_rows_bits, current_uncertainty).reveal()) 

true_rows_bits2 = partition(training_data, question(0, 2),numeric_list)[0]
print_ln('La ganancia de informacion al partirlo por red: %s',info_gain(true_rows_bits2, current_uncertainty).reveal())

#------------------------FUNCION element_search --------------------
#Funcion para buscar si un elemento a esta en una lista
def element_search(a,lista):
    n=len(lista)
    result=MemValue(sint(0))
    @for_range(n)
    def range_body(i):
        result.write(result+(lista[i] == a))
    @if_e((result.read()==0).reveal())
    def block():
        return sfix(0)
    @else
    def block():
        return sfix(1)

#---------------------FUNCION best_split_gini-------------------
#Funcion para encontrar el mejor corte basandose en la impureza de gini
#Encontrar la mejor pregunta que hacer yendo uno por uno por todas las variables
#y todos los valores posibles y comparando la ganancia de informacion.

def best_split_gini(data,valores_y,partition_list,nodo,p):
    best_gain = sfix.Array(1)  # aqui guardamos la mayor ganancia de infomracion
    #Lo inicializamos en 0
    best_gain[0]=sfix(0)

    best_question = [0,0]  # aqui guardamos el valor y la columna que hacen la mejor pregunta
    #Por el momento lo inicializamos en [0,0], aunque eso es una pregunta de por si.
          
    current_uncertainty = gini_2(data,valores_y,partition_list,nodo)
    n_variablesX = len(data[0]) - 1  # number of columns

    @for_range(n_variablesX)
    def range_body(col):
        #Para cada variable explicativa, x
        #Sacamos la lista de 1s y 0s con los valores repetidos y no repetidos de la columna
        
        #Pero no hay que sacarlos del data entero si no del data correspondiente al nodo especifico que estmaos mirando
        
        valores=valores_repetidos(data,col)

        @for_range(len(valores))
        def range_body(i):
        #Miramos el valor correspondiente a los que tienen un 0 en valores unicos
            @if_e((valores[i]==0).reveal()==0) 
            def block():
                valor=data[i][col]
                #para cada valor que puede tomar se realiza la pregunta
                pregunta = question(col, valor)

                # Intentar dividir el dataset mediante esa pregunta
                partition_list = partition(data, partition_list,pregunta, numeric_list,nodo,iteracion,p)
                #Te devuelve una lista de numeros indicando si es true o false
                #SACAR EL NUMERO QUE CORRESPONDE A LOS DE FALSE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
                es_false=nodo+p+2**iteration

                #SACAR EL NUMERO QUE CORRESPONDE A LOS DE TRUE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
                es_true=nodo+p+1+2**iteration


                # Si el corte no divide el dataset saltarselo. Para ello, miramos que los numeros
                # esten al menos una vez en partition_list

                @if_e(or_((element_search(es_false,partition_list)==0).reveal(),(element_search(es_true,partition_list)==0).reveal()))
                def block():
                    continue
                @else_
                def block():    
                    # Calcular la ganancia de informacion con ese corte
                    gain = info_gain(partition_list, current_uncertainty)

                    @if_e((gain>=best_gain).reveal()==1)
                    def block():
                        best_gain[0]=gain
                        best_question=pregunta
                    @else_
                    def block():
                        best_question[0]=best_gain[0]
                        best_question=best_question
            @else_
            def block():
                #Los que tienen un 1 no hace falta mirarlos, porque significa que estan
                #repetidos y que por lo tanto ya se han mirado 
                continue

    return best_gain[0], best_question


#-----------------Voy a cambiarlo mucho, hago otro para no liarlo mas todavia------------
#Lo malo de esto es que mirara varias veces las preguntas repetidas, pero por lo demas creo que esta bien 

def best_split_gini_2(data,valores_y,partition_list,nodo,numeric_list,iteration,p):

    best_gain = sfix.Array(1)  # aqui guardamos la mayor ganancia de infomracion
    #Lo inicializamos en 0
    best_gain[0]=sfix(0)

    best_question = [0,0]  # aqui guardamos el valor y la columna que hacen la mejor pregunta
    #Por el momento lo inicializamos en [0,0], aunque eso es una pregunta de por si.
          
    current_uncertainty = gini_2(data,valores_y,partition_list,nodo)

    n_variablesX = len(data[0]) - 1  # numero de columnas, es decir numero de variables explicativas X

    @for_range(n_variablesX)
    def range_body(col):
        #Para cada variable explicativa, x
        #miramos todos los posibles valores que toma esa columna
        @for_range(len(data)):
        def range_body(j):
            #Miramos los que en la partition list tengan justo ese numero de nodo
            #Es decir,los que realmente pertenecen al data del nodo que queremos dividir
            @if_e((partition_list[j]==nodo).reveal())
            def block():
                valor=data[j][col]
                #para cada valor que puede tomar se realiza la pregunta
                pregunta = question(col, valor)

                # Intentar dividir el dataset mediante esa pregunta
                partition_list = partition(data, partition_list,pregunta, numeric_list,nodo,iteration,p)
                #Te devuelve una lista de numeros indicando si es true o false
                
                #SACAR EL NUMERO QUE CORRESPONDE A LOS DE FALSE DEPENDIENDO DE LA ITERACION Y DEL NODO:
                es_false=nodo+p+2**iteration

                #SACAR EL NUMERO QUE CORRESPONDE A LOS DE TRUE DEPENDIENDO DE LA ITERACION Y DEl NODO:
                es_true=nodo+p+1+2**iteration


                # Si el corte no divide el dataset saltarselo. Para ello, miramos que los numeros
                # esten al menos una vez en partition_list

                @if_e(or_((element_search(es_false,partition_list)==0).reveal(),(element_search(es_true,partition_list)==0).reveal()))
                def block():
                    best_gain[0]=best_gain[0]
                    best_question=best_question
                @else_
                def block():    
                    # Calcular la ganancia de informacion con ese corte
                    gain = info_gain(data,valores_y,partition_list, current_uncertainty,nodo,iteration,n)

                    @if_e((gain>=best_gain).reveal()==1)
                    def block():
                        best_gain[0]=gain
                        best_question=pregunta
                    @else_
                    def block():
                        best_gain[0]=best_gain[0]
                        best_question=best_question
            @else_
            def block():
                #Los que no aparecen en el data que queremos dividir no hay que mirarlos 
                best_gain[0]=best_gain[0]
                best_question=best_question

    return best_gain[0], best_question


#-------------------------------FUNCION build_tree----------------------------- 
#Primero se intenta dividir el dataset por cada uno de sus atributos unicos y 
#calcular la ganancia de informacion y la pregunta que lo produce

#Caso base: no hay mas ganancia de informacion, porque no se pueden realizar mas preguntas
#se devuelve el leaf 

#Si no entramos en el caso base es porque se ha encntrado una particion util


#Si todas las variables son cualitativas si que es facil saber cual es el maximo numero de preguntas
#Si hay variables cuantitativas el numero de preguntas no es fijo, porque se pueden ir haciendo cortes cada vez mas fijandonos

#En general como maximo se pueden hacer no de filas -1 preguntas: q preguntas
#Se puede crear una matriz de tamaño qx2, para ir guardando las preguntas



#data va a ser siempre el initial data, es decir: training_data
def build_tree(data,valores_y,max_iteration):

    q=len(data)-1
    preguntas=sfix.Matrix(q,2)

    #Guardamos en una variable el numero de preguntas que se hacen,
    #asi unicamente se leera de la lista preguntas el numero de preguntas que realmente se han realizado
    num_preguntas=MemFix(sfix(0))

    iteration= MemValue(sint(0))

    #partition_list en un principio sera una lista llena de 0s, porque todos pertenecen al nodo 0
    
    partition_list=sfix.Array(len(data))
    @for_range(n)
    def range_body(i):
        partition_list[i]=sfix(0)

    #El nodo empieza en cero y se van mirando todos los nodos
    nodo=MemFix(sfix(0))

    #Con esto indicamos si en esta interacion se ha realizado alguna particion o no
    #En caso de que no se haya relizado ninguna no se debe seguir con la siguiente iteracion
    hace_particion=MemFix(sfix(0))

    gain, question = best_split_gini_2(data,valores_y,partition_list,nodo,numeric_list,iteration,p)

    #Para ir guaradando las preguntas:
    indice=sfix.Array[1]
    indice[0]=sfix(0)

    #Al principio p es cero
    p=MemFix(sfix(0))

    @if_e gain!=0
    def block():
        preguntas[indice[0]][0]=question[0]
        preguntas[indice[0]][1]=question[1]
        indice[0]=indice[0]+1
        partition_list=partition(data,partition_list, question,numeric_list,nodo,iteration,p)
        hace_particion.iadd(1)
    @else_
    def block():
         partition_list=partition_list

    #Ahora hay que separar en 2^iteration 
    iteration.iadd(1)

    
    @do_while
    def f():

        #Para comprobar que en esta iteracion se ha hecho alguna particion
        #en caso contrario parar el while
        hace_particion=sfix(0)

        #Al pricipio de cada iteracion p vuelve a ser cero:
        p=p.write(sfix(0))

        @for_range(2**iteration)
        def range_body(i):
            nodo.iadd(1)
            #Si el nodo madre no esta en partition_list, obviamente no hay que partirlo
            esta_nodo=element_search(nodo,partition_list)
            @if_e((esta_nodo.reveal()==1))
            def block():
                gain, question = best_split_gini_2(data,valores_y,partition_list,nodo,numeric_list,iteration,p)
                @if_e gain!=0
                def block():
                    preguntas[indice[0]][0]=question[0]
                    preguntas[indice[0]][1]=question[1]
                    indice[0]=indice[0]+1
                    partition_list=partition(data,partition_list, question,numeric_list,nodo,iteration,p)
                    hace_particion.iadd(1)
                @else_
                def block():
                    partition_list=partition_list
            @else_
            def block():
                partition_list=partition_list
            #Despues de cada nodo a p se le suma uno:
            p.iadd(1)
            
        iteration.iadd(1)

        

        return and_(lambda:iteration<=max_iteration, lambda:hace_particion.read()!=0)

    #Aparte de la iteracion, tambien se tiene que parar cuando ya no hay mas nodos madres que mirar, es decir
    #si por ejemplo en la iteracion 2 se intentan dividir los nodos 5 y 6 pero no aumenta el gain
    #no tiene sentido hacer la iteracion 3. Algo que indique que la partition_list ha cambiado respecto a la iteracion anterior

    #El loop para cuando el return value es cero o false
    #Aqui tambien pasa que las local variables no pueden ser modificdas y hay que 
    #utilizar memvalues 


    return preguntas,num_preguntas,partition_list


#COMPROBAR:
preguntas,numero_preguntas,partition_list=build_tree(training_data,valores_y,max_iteration=10)

print_ln('\n RESULTADO DEL ARBOL DE DECISION: \n)


@for_range(numero_preguntas.read.reveal())
def range_body(i):
    print_ln('La pregunta %s es:',i)
    @for_range(2)
    def reange_body(j):
        print_str('%s ', preguntas[i][j].reveal()) #Asi se imprime toda la pregunta en una linea

print_ln('\n La lista indicando cada fila a que nodo pertenece es: \n')
@for_range(len(training_data))
def range_body(i):
    print_str('%s ', partition_list[i].reveal()) #Asi se imprime toda la lista en una linea
        

#-------------------------FUNCION print_tree-----------------------
#Tiene mas sentido cuando leaf es una clase en vez de una funcion
#Hay que encontrar una forma de preguntar si node es un leaf 
#En este caso leaf no es una clase y por eso se le añade el 12346 como ultimo 
#elemento para poder identificarlo. 
#En python se imprime bien, pero aqui se va a imprimir mal todo lo de los espacios

def print_tree(node):

    @if_e((node[-1]==123456).reveal()==1)
    def block():
        #print_ln(' Predict', node[0:-1].reveal())
        print_ln(' Predict')
        for i in range(len(node)-1):
            print_ln('%s',node[i].reveal())
        return 
        
    @else_
    def block():
        p=0
        #por poner algo
        
    # Imprimir la pregunta en este nodo 
    question=node[0]
    # print_ln(' %s', node[0].reveal())
    print_ln(' columna %s', node[0][0].reveal())
    print_ln(' valor %s', node[0][1].reveal())

    # Llamar a esta funcion recursivamente en el true branch
    
    #node.true_branch=node[1]
    print_tree(node[1])

    # Call this function recursively on the false branch
    print_ln(' -->False:')
    #node.false_branch=node[2]
    print_tree(node[2])

#-------------Comprobamos que funciona print_tree-----------------------
my_tree = build_tree(training_data)
print_tree(my_tree)

#-------------------------FUNCION classify-------------------------------
#Caso base: si se ha llegado a un leaf
#En otro caso decidir si seguir el true-branch o el false-branch. 
#Comparar el valor del nodo con el que tenemeos en el ejemplo que estamos considerando


def classify(row, node,numeric_list):

    # Caso base
    @if_e((node[-1]==123456).reveal()==1)
    def block():
        return node[0:-1]   
    @else_
    def block():
        p=0
        #por poner algo

    @if_e((match_question(node[0],row,numeric_list[node[0][0]])).reveal()==1)
    def block():
        #node.true_branch=node[1]
        return classify(row, node[1])  
    @else_
    def block():
        #node.true_branch=node[1]
        return classify(row, node[2])


#------------------------Comprobamos que funciona classify--------------
#Va a salir una lista con los elementos que tiene, es decir 
#En su caso sale: {'Apple': 1}
#En nuestro caso: [1, 0, 0]
lista_calssify=classify(training_data[0], my_tree, numeric_list)
categorias_y=valores_unicos(initial_data, -1)
for i in range(len(categorias_y)):
    print_ln('Classify:')
    print_ln('%s',lista_calssify[i].reveal())

#---------------------------FUNCION porcentajes_classify----------------

def porcentajes_classify(lista_classify):
    total=sfix.Array(1)
    total[0]=sfix(0)
    for i in range(len(lista_calssify)):
        total[0]=total[0]+lista_calssify[i]
    probabilidades=[]
    for i in range(len(lista_calssify)):
        probabilidades.append(lista_calssify[i]/total[0]*100)
    return probabilidades

#--------------DATOS DE VALIDACION------------------------------


#testing_data = [
#    ['Green', 3, 'Apple'],
#    ['Yellow', 4, 'Apple'],
#    ['Red', 2, 'Grape'],
#    ['Red', 1, 'Grape'],
#    ['Yellow', 3, 'Lemon']]


#testing_data  = [[0, 3, 0],
#                 [1, 4, 0],
#                 [2, 2, 1],
#                 [2, 1, 1],
#                 [1, 3, 2]]

#Esto pensar como vendra dado del .txt:
testing_data=sfix.Matrix(5,3)

testing_data[0][0]=sfix(0)
testing_data[0][1]=sfix(3)
testing_data[0][2]=sfix(0)

testing_data[1][0]=sfix(1)
testing_data[1][1]=sfix(4)
testing_data[1][2]=sfix(0)

testing_data[2][0]=sfix(2)
testing_data[2][1]=sfix(2)
testing_data[2][2]=sfix(1)

testing_data[3][0]=sfix(2)
testing_data[3][1]=sfix(1)
testing_data[3][2]=sfix(1)

testing_data[4][0]=sfix(1)
testing_data[4][1]=sfix(3)
testing_data[4][2]=sfix(2)

for i in range(len(testing_data)):
    row=testing_data[i]
    print_ln('Actual: ')
    print_ln('%s',row[-1].reveal())
    print_ln('Predicted: ')
    probabilidades=porcentajes_classify(classify(row, my_tree, numeric_list))
    for i in range(len(probabilidades)):
        print_ln('%s %', probabilidades[i].reveal())
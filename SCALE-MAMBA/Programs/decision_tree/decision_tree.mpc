
#-----------------------------ARBOL DECISION DE CLASIFICACION------------------------------

#La variable Y es cualitativa. 
#Se podria hacer algo similar siendo Y es cuantitativa para construir un arbol de regresion 

#ARBOLES DE REGRESION:
#   - Variable dependiente es continua
#   - Valores de los nodos terminales se reducen a la media de las observaciones en esa region.

#ARBOLES DE CLASIFICACION:
#   - Variable dependiente es categorica
#   - El valor en el nodo terminal se reduce a la moda de las observaciones del conjunto
#   de entrenamiento que han caido en esa region. Es decir, el elemento mas repetido.


#CODIGO DE PYTHON INSPIRADO EN:
#https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb
#(Pero este utiliza clases y listas dinamicas)
#Bastantes cambios realizados para adaptarlo a SCALE-MAMBA

#En este codigo faltaria el preprocesamiento de los datos donde se 
#se separa el conjunto de datos en un conjunto de entrenemiento y en
#otro de validacion

#Para realizar este ejemplo directamente se dan los datos de 
#entrenamiento y validacion por separado

#Los datos se guardan en una matriz data
#   Las primeras columnas representan cada una de las variables explicativas X1,...,Xn
#   La ultima columna representa la variable respuesta Y

#El arbol de clasificacion consiste en eparar los datos basandose en los valores de las
#variables X1,...,Xn e identificar la variable que crea los conjuntos mas homogeneos y 
#que a su vez son heterogeneos entre ellos (fijandonos en los valors de Y)


#DESVENTAJAS:
#   Sobreajuste (Se puede solucionar metiendo varios arboles de decision en un Random Forest)
#   Perdida de informacion al categorizar variables continuas
#   Precision: metodos como SVM y clasificadores tipo ensamblador a menudo tienen tasas de error 30% mas bajas que CART
#   Inestabilidad: un minimo cambio en los datos puede modificar ampliamente la estructura del arbol.
#Por lo tanto la interpretacion no es tan directa como parece.

#tipo: {"regresion","clasificacion"}
#criterio : {"gini", "entropy"}. La funcion para medir la calidad de una division.
#Seleccionar que variable elegir para obtener la mejor division. Dos opciones:
#               Calculando la impureza de Gini
#               Entropia: para la ganancia de informacion
#               Este parametro es especifico de los arboles de decisiones


#---------------Ejemplo utilizando la impureza de gini y un arbol de clasificacion-------------


#CONJUNTO DE ENTRENAMIENTO:

#training_data = [['Green', 3, 'Apple'],
#                 ['Yellow', 3, 'Apple'],
#                 ['Red', 1, 'Grape'],
#                 ['Red', 1, 'Grape'],
#                 ['Yellow', 3, 'Lemon']]

#X1: color (cualitativa)
#X2: diameter (cuantitativa)
#Y: label (cualitativa)

#En SCALE-MAMBA para trabajar con variable cualitativa, como no se puede usar
#string lo guardamos como numeros, pero hay que indicar de alguna forma que en
#realidad es cualitativa. 

#Asignamos los numeros para color:
#   'Green':0
#   'Yellow':1
#   'Red':2

#Asignamos los numero para label:
#   'Apple':0
#   'Grape':1
#   'Lemon':2

#Todo esto habra que hacerlo en el preprocesamiento y recibir directamente 
#del .txt datos numericos, de hecho por el momento solo sabemos recibir sint
#y a lo sumo transformalo a sfix.

#Indicamos mediante una lista si las variables X1,X2 e Y son cuantitvas o cualitativas
#en ese orden.
#   Si es cualitativa, no es numerica:0
#   Si es cuantitativa, es numerica:1
#Esto tambien habra que hacerlo en el preprocesamiento publicamente, no creo que sea un problema.

#numeric_list=[0,1,0]

numeric_list=sfix.Array(3)
numeric_list[0]=sfix(0)
numeric_list[1]=sfix(1)
numeric_list[2]=sfix(0)

#Por lo tanto nuestro data sera:

#training_data = [[0, 3, 0],
#                 [1, 3, 0],
#                 [2, 1, 1],
#                 [2, 1, 1],
#                 [1, 3, 2]]

#Esto hay que pensar como vendra dado del .txt:
#EL numero de variables y el numero de filas,observaciones, debe ser algo conocido.

training_data=sfix.Matrix(5,3)

training_data[0][0]=sfix(0)
training_data[0][1]=sfix(3)
training_data[0][2]=sfix(0)

training_data[1][0]=sfix(1)
training_data[1][1]=sfix(3)
training_data[1][2]=sfix(0)

training_data[2][0]=sfix(2)
training_data[2][1]=sfix(1)
training_data[2][2]=sfix(1)

training_data[3][0]=sfix(2)
training_data[3][1]=sfix(1)
training_data[3][2]=sfix(1)

training_data[4][0]=sfix(1)
training_data[4][1]=sfix(3)
training_data[4][2]=sfix(2)

#Esto no tiene por que ser secreto y facilita muchos las cosas
valores_y=sfix.Array(3)
valores_y[0]=sfix(0)
valores_y[1]=sfix(1)
valores_y[2]=sfix(2)

#Esto tampoco tiene que ser secreto y facilita mucho las cosas
#Los valores de las variables cualitativas se dan uno a uno:
#Sirve para la funcion best_split_gini_random
valores_color=sfix.Array(3)
valores_color[0]=sfix(0)
valores_color[1]=sfix(1)
valores_color[2]=sfix(2)


#-----------------------------------------------------------------------------------------
#                                       FUNCIONES
#-----------------------------------------------------------------------------------------

#-------------------------------- FUNCION element_search ---------------------------------
#Funcion para buscar si un elemento a esta en un Array lista

def element_search(a,lista):
    n=len(lista)
    result=MemValue(sint(0))
    #Aqui guardamos si son iguales o no:
    equal=sfix.Array(1)
    equal[0]=sfix(0)

    @for_range(n)
    def range_body(i):
        result.write(result+(lista[i] == a))

    @if_e((result.read()==0).reveal())
    def block():
        equal[0] = sfix(0)
    @else_
    def block():
        equal[0] = sfix(1)
    
    return equal[0]

#-------------------------------Comprobacion element_search---------------------------------
#Comprobamos que el 0 esta en valores_y
print_ln('\nEl 0 esta en valores_y: %s',element_search(sfix(0),valores_y).reveal())

#Comprobamos que el 4 no esta en valores_y
print_ln('\nEl 4 no esta en valores_y: %s',element_search(sfix(4),valores_y).reveal())

#....FUNCIONA CORRECTAMENTE...

#------------------------------------- FUNCION question -------------------------------------
#Es una lista con dos atributos:
#   columna: columna de donde se saca la pregunta
#   valor: el valor que toma en dicha columna

#Es decir, para hacer las preguntas:
#   Es color==green? Se hace question(0,0)
#                    colos es la columna 0
#                    green es el valor 0

#   Es diameter >=3? Se hace question (1,3)
#                    diameter es la columna 1

def question(columna,valor):
    pregunta=sfix.Array(2)
    pregunta[0]=columna
    pregunta[1]=sfix(valor)
    return pregunta

#---------------------------------- FUNCION match_question -----------------------------------
#Se le da una pregunta y una fila como argumento
#El resultado es si esa fila responde a la pregunta o no
#Tambien hay que darle como argumento numeric_list para que sepa si es una pregunta sobre una
#variable cuantitativa o sobre una cualitatita.


def match_question(question,row,numeric_list):

    #Primero comparamos si la pregunta la hacemos sobre una variable categorica o numerica
    #question[0] guarda la columna, es decir, la variable 

    variable=question[0]
    #numeric=numeric_list[variable] #El problema es que para esto variable tiene que ser un entero
    #valor=row[variable]

    numeric=sfix.Array(1)
    numeric[0]=sfix(0)

    valor=sfix.Array(1)
    valor[0]=sfix(0)

    @for_range(len(numeric_list))
    def range_body(i):
        @if_e((variable==i).reveal()==1)
        def block():
            numeric[0]=numeric_list[i]
            valor[0]=row[i]
        @else_
        def block():
            numeric[0]=numeric[0]
            valor[0]=valor[0]
            

    

    result=sfix.Array(1)
    result[0]=sfix(0)

    @if_e((numeric[0]==1).reveal()==1)
    def block():
        @if_e((valor[0] >= question[1]).reveal()==1)
        def block():
            result[0]=sfix(1)
        @else_
        def block():
            result[0]=sfix(0)
    @else_
    def block():
        @if_e((valor[0] == question[1]).reveal()==1)
        def block():
            result[0]=sfix(1)
        @else_
        def block():
            result[0]=sfix(0)

    return result[0]

#------------------------------- Comprobar que funciona match_question ---------------------------
#Cogemos la pregunta Is color==green
q=question(0,sfix(0))

#   Cogemos la fila 0 como ejemplo, es decir la fila [0, 3, 0]. La repuesta debe ser true
example1=training_data[0]
print_ln('\nPara la fila cero el color SI  es verde: %s',match_question(q,example1,numeric_list).reveal())

#   Cogemos la fila 2 como ejemlo, es decir la fila [2, 1, 1]. La respuesta deber ser false
example2=training_data[2]
print_ln('\nPara la fila dos el color NO es verde: %s',match_question(q,example2,numeric_list).reveal())

#Para la pregunta is diameter >=3
q2=question(1,sfix(3))

#   Cogemos la fila 0 como ejemplo, es decir la fila [0, 3, 0]. La repuesta debe ser true
example3=training_data[0]
print_ln('\nPara la fila cero el diametro SI  es mayor o igual que 3: %s',match_question(q2,example3,numeric_list).reveal())

#   Cogemos la fila 2 como ejemlo, es decir la fila [2, 1, 1]. La respuesta deber ser false
example4=training_data[2]
print_ln('\nPara la fila dos el diametro NO es mayor o igual que tres : %s',match_question(q2,example4,numeric_list).reveal())

#....FUNCIONA CORRECTAMENTE...


#----------------------------------------- FUNCION gini ------------------------------------------
#Sirve para calcular la impureza de gini
#Como de frecuente es que un elemento elegido de forma random sea clasificado incorrectamente
    #si se elige el tipo de forma random, es decir, si tienes un bol con naranjas, mandarinas y manzanas
    #y otro bol con papeles en los que pone naranja, mandarina o manzana
    #Se elige de cada bol un elemento de forma random.
    #La frecuencia de que no coincida se mide con la impureza de gini.
#formula sacada de : https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity

#En realidad, no tendremos una lista con el data dividido, por lo que hay la funcion de gini toma el data entero 
#y la lista partition_list que le indica como esta dividido para dividirlo dentro de la funcion 

def gini(data,valores_y,partition_list,nodo):
    
    #Numero de categorias de la variable y
    #pero puede que las m no esten en el subdata que estamos mirando
    m=len(valores_y) 

    #En counts lo que tenemos que guardar es por cada tipo diferente de ultima variable cuantos hay
    #En caso de que ese variable no este en nuestro conjunto sera cero 
    #Si es cero no influye porque p_i sera 0 y no se le resta nada a gini
    counts=sfix.Array(m) 
    
    #Guadamos en una variable el numero de elementos de ese nodo:
    q=sfix.Array(1)
    q[0]=sfix(0)

    #Miramos de entre todas las filas del data las que estan en nuestro nodo
    @for_range(len(data))
    def range_body(i):
        #Miramos los que en la partition_list tengan justo ese numero de nodo
        @if_e((partition_list[i]==nodo).reveal()==1)
        def block():
            #Sumamos uno al numero de elementos de ese nodo:
            q[0]=q[0]+1
            #Cogemos el valor que toma esta fila en la variable Y
            label=data[i][-1]

            #Ahora miramos cual de todas las m posibilidades de Y es y le sumamos uno:
            @for_range(m)
            def range_body(j):
                categoria=valores_y[j]

                #Si el valor de label es igual al de la categoria sumamos counts[j]+1
                @if_e((label==categoria).reveal()==1)
                def block():
                    counts[j]=counts[j]+1
                @else_
                def block():
                    counts[j]=counts[j]
        @else_
        def block():
            nada=0
            #no hacemos nada
 
    
    #La formula es:
    #I=1- sum(i=1,2,...,m)p_i^2
    #suponiendo que m es el numero de clases y p_i la fraccion de items clasificados como clase i
    
    impurity=sfix.Array(1)
    impurity[0]=sfix(1)

    @for_range(m)
    def range_body(i):
        p_i = counts[i] / q[0]
        impurity[0] =impurity[0] - p_i*p_i

    return impurity[0]

#-------------------------------- Comprobamos que funciona gini -----------------------------------
#Para hacer las pruebas definimos que todos estan en el nodo que queremos, es decir:
partition_list_iter0=sfix.Array(len(training_data))
@for_range(len(training_data))
def range_body(i):
    partition_list_iter0[i]=sfix(0)

#El nodo es cero
nodo=MemFix(sfix(0))

#Ejemplo 0
print_ln('\ngini del training data: %s', gini(training_data,valores_y, partition_list_iter0, nodo).reveal())

#Ejemplo 1
#Probamos a hacer el gini del nodo 1 manualmente:
partition_list_iter1=sfix.Array(5)
partition_list_iter1[0]=sfix(2)
partition_list_iter1[1]=sfix(2)
partition_list_iter1[2]=sfix(1)
partition_list_iter1[3]=sfix(1)
partition_list_iter1[4]=sfix(2)

print_ln('\ngini del nodo 1: %s', gini(training_data,valores_y, partition_list_iter1,nodo=sfix(1)).reveal())

#Ejemplo 2
print_ln('\ngini del nodo 2: %s', gini(training_data,valores_y, partition_list_iter1,nodo=sfix(2)).reveal())

#....FUNCIONA CORRECTAMENTE...



#-----------------------------PARA REALIZAR 2^EXPONENTE---------------------------------
from Compiler import mpc_math

sfloat.vlen = 15 # Length of mantissa in bits
sfloat.plen = 10 # Length of exponent in bits
sfloat.kappa = 4 # Statistical security parameter for floats



#-------------------------------------- FUNCION info_gain -----------------------------------------
# La uncertainity del nodo de entrada(es decir el indice de gini) menos las impurezas ponderaas de los dos hijos
# En este caso nodo es el nodo anterior a relizar la partition list
# Es decir partition list es la de los nodos hijos, peor nodo es el nodo madre, que ya no estara en partition_list

def info_gain(data,valores_y,partition_list, current_uncertainty,nodo,iteration,p):
    exp2_iter=mpc_math.exp2_fx(iteration)

    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE FALSE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
    es_false=nodo + exp2_iter + p

    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE TRUE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
    es_true=nodo + exp2_iter + p + sfix(1)

    trues=MemFix(sfix(0))

    falses=MemFix(sfix(0))

    @for_range(len(partition_list))
    def range_body(i): 
        @if_e((partition_list[i]==es_true).reveal()==1)
        def block():
            trues.iadd(1)
        @else_
        def block():
            trues.iadd(0)
        
        @if_e((partition_list[i]==es_false).reveal()==1)
        def block():
            falses.iadd(1)
        @else_
        def block():
            falses.iadd(0)

    prob = trues.read() / (trues.read()+falses.read())

    
    resultado = current_uncertainty - prob * gini(data,valores_y,partition_list,es_true) - (1 - prob) * gini(data,valores_y,partition_list,es_false)
    
    return resultado

#-------------------------------- Comprobamos que funciona info_gain -----------------------------
current_uncertainty = gini(training_data,valores_y,partition_list_iter0,nodo=sfix(0))
print_ln('\nLa current_uncertainty: %s', current_uncertainty.reveal())

print_ln('\nLa ganancia de informacion al partirlo diameter==3, es decir la primera iteracion es:  %s',info_gain(training_data,valores_y,partition_list_iter1,current_uncertainty,nodo=sfix(0),iteration=sfix(0),p=sfix(0)).reveal()) 

#....FUNCIONA CORRECTAMENTE...


#-------------------------------------- FUNCION partition -----------------------------------------
#Se encarga de partir el dataset en dos datasets dependiendo de la respuesta a la pregunta
#Para cada linea de un dataset, comprobar si la respuesta a la pregunta es verdadero o falso 
#   Si es verdadero se agrega la linea a true_rows
#   Si es falsa se agrega la linea a false_rows

#No devuelve una lista para true y otra para false, devuelve una unica lista que se va actualizando
#donde se indica cada fila a que nodo pertenece

# Dependiendo de la iteration  y el nodo haremos que ponga unos numeros o otros

def partition(data, partition_list,question,numeric_list,nodo,iteration,p):

    partition_nueva=sfix.Array(len(data))

    exp2_iter=mpc_math.exp2_fx(iteration)

    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE FALSE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
    nodo_false=nodo + exp2_iter + p

    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE TRUE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
    nodo_true=nodo + exp2_iter + p + sfix(1)
    

    @for_range(len(data))
    def range_body(i): 
        #Comprobamos si esa fila pertenece al nodo que queremos dividir
        @if_e((partition_list[i]==nodo).reveal())
        def block():
            #Si pertenece al nodo cogemos la linea y le hacemos la pregunta
            row=data[i]
       
            @if_e((match_question(question,row,numeric_list)).reveal()==1)
            def block():
                partition_nueva[i]=nodo_true

            @else_
            def block():
                partition_nueva[i]=nodo_false

        #Si no pertenece al nodo que queremos dividir se queda igual
        @else_
        def block():
            partition_nueva[i]=partition_list[i]

 
    return partition_nueva

#-------------------------------- Comprobamos que funciona partition ---------------------------
#Iteracion0 a 1
partition_list_result = partition(training_data,partition_list_iter0,question(1,sfix(3)),numeric_list,nodo=sfix(0),iteration=sfix(0),p=sfix(0))

print_ln('\nLas lineas que pertencen a true (son las que estan en la posicion del indice con valor 2): ')

print_ln('\nLas lineas que pertencen a false (son las que estan en la posicion del indice con valor 1): ')

for i in range(len(partition_list_result)):
    print_str(' %s',partition_list_result[i].reveal()) #Imrimimos todo en una linea

#Iteracion 1 a 2
partition_list_result2 = partition(training_data,partition_list_result,question(0,sfix(1)),numeric_list,nodo=sfix(2),iteration=sfix(1),p=sfix(1))

print_ln('\nLas lineas que pertencen a true (son las que estan en la posicion del indice con valor 6): ')

print_ln('\nLas lineas que pertencen a false (son las que estan en la posicion del indice con valor 5): ')

for i in range(len(partition_list_result2)):
    print_str(' %s',partition_list_result2[i].reveal()) #Imrimimos todo en una linea

#....FUNCIONA CORRECTAMENTE...


#--------------------------------- FUNCION best_split_gini ----------------------------------------
#Funcion para encontrar el mejor corte basandose en la impureza de gini
#Encontrar la mejor pregunta que hacer yendo uno por uno por todas las variables
#y todos los valores posibles y comparando la ganancia de informacion.

#Lo malo de esto es que mirara varias veces las preguntas repetidas, pero por lo demas creo que esta bien 

def best_split_gini(data,valores_y,partition_list,nodo,numeric_list,iteration,p):
    
    # aqui guardamos la mayor ganancia de infomracion
    best_gain = sfix.Array(1)  
    #Lo inicializamos en 0
    best_gain[0]=sfix(0)

    # aqui guardamos el valor y la columna que hacen la mejor pregunta
    best_question=sfix.Array(2)
    best_question[0]=sfix(0)
    best_question[1]=sfix(0)
    #Por el momento lo inicializamos en [0,0], aunque eso es una pregunta de por si.
    #(no influye, si es la mejor esta bien y si no lo es se reemplazara)
          
    current_uncertainty = gini(data,valores_y,partition_list,nodo)

    n_variablesX = len(data[0]) - 1  # numero de columnas, es decir numero de variables explicativas X

    #Definimos un Array para guardar la nueva partition:
    partition_nueva=sfix.Array(len(data))

    @for_range(n_variablesX)
    def range_body(col):

        #Para cada variable explicativa, x
        #miramos todos los posibles valores que toma esa columna
        @for_range(len(data))
        def range_body(j):

            #Miramos los que en la partition list tengan justo ese numero de nodo
            #Es decir,los que realmente pertenecen al data del nodo que queremos dividir
            @if_e((partition_list[j]==nodo).reveal())
            def block():
                valor=data[j][col]
                #para cada valor que puede tomar se realiza la pregunta
                pregunta = question(col, valor)

                # Intentar dividir el dataset mediante esa pregunta
                partition_nueva = partition(data, partition_list,pregunta,numeric_list,nodo,iteration,p)
                #Te devuelve una lista de numeros indicando si es true o false
                
                exp2_iter=mpc_math.exp2_fx(iteration)

                #SACAR EL NUMERO QUE CORRESPONDE A LOS DE FALSE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
                es_false=nodo + exp2_iter + p

                #SACAR EL NUMERO QUE CORRESPONDE A LOS DE TRUE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
                es_true=nodo + exp2_iter + p + sfix(1)

                # Si el corte no divide el dataset saltarselo. Para ello, miramos que los numeros
                # esten al menos una vez en partition_list. Si lo divide estaran los dos numeros,
                # por lo que con mirar que esta uno de los dos vale, miramos por ejemplo que este es_false
          
                @if_e((element_search(es_false,partition_nueva)==0).reveal())
                def block():
                    #Si no esta es porque no lo ha dividio, no actualizamos
                    best_gain[0]=best_gain[0]
                @else_
                def block():    
                    # Calcular la ganancia de informacion con ese corte
                    gain = info_gain(data,valores_y,partition_nueva, current_uncertainty,nodo,iteration,p)
                                     

                    @if_e((gain>=best_gain[0]).reveal()==1)
                    def block():
                        best_gain[0]=gain
                        best_question[0]=pregunta[0]
                        best_question[1]=pregunta[1]
                    @else_
                    def block():
                        best_gain[0]=best_gain[0]
            @else_
            def block():
                #Los que no aparecen en el data que queremos dividir no hay que mirarlos 
                best_gain[0]=best_gain[0]

    return best_gain[0], best_question


#----------------------------------- Comprobacion best_split_gini -------------------------------------
#La mejor pregunta para hacer en la iteracion cero al nodo 0 es [1,3]:

best_gain0, best_question0 = best_split_gini(training_data, valores_y, partition_list_iter0, nodo=sfix(0), numeric_list=numeric_list, iteration=sfix(0), p=sfix(0))

print_ln('\nLa mayor ganancia de informacion en el nodo 0 es: %s',best_gain0.reveal())

print_ln('\nY se consigue con la pregunta: \n')

@for_range(2)
def range_body(i):
    print_str(' %s',best_question0[i].reveal()) #Para imprimir la pregunta en la misma linea

#La mejor pregunta para hacer en la iteracion uno al nodo 2 es [0,1]:
best_gain2, best_question2 = best_split_gini(training_data,valores_y,partition_list_iter1,nodo=sfix(2),numeric_list=numeric_list,iteration=sfix(0),p=sfix(1))

print_ln('\nLa mayor ganancia de informacion en el nodo 2 es: %s',best_gain2.reveal())

print_ln('\nY se consigue con la pregunta: \n')

@for_range(2)
def range_body(i):
    print_str(' %s',best_question2[i].reveal()) #Para imprimir la pregunta en la misma linea

#....FUNCIONA CORRECTAMENTE...

#--------------------------------- FUNCION best_split_gini_random ----------------------------------------

#Meterle n_variablesX a mano para que sea un entero y no un sfix

#Funcion para encontrar el mejor corte basandose en la impureza de gini
#Encontrar la mejor pregunta que hacer eligiendo de forma random las variables que vamos a mirar
#Este mecanisimo se utiliza para controlar el overfitting. Es similar a la tecnica de random forest. 
#La diferencia es que en el Random Forest se separa de forma random tambien el data y se hacen varios arboles 
#Aunque fijemos el numero de variables a 3, segun se vaya a lo largo del arbol acabaran apareciendo todas las variables 

#Tambien voy a fijar el numero de valores que se fijan en cada variable (esto creo que no lo hace scikit-learn), 
#pero hace que el proceso sea mucho mas rapido y actua de forma similar. 
#Esto solo para las variables cuantitativas

#Para las variables cualitativas se mira cada categoria en una lista que le pasamos

#Por lo tanto los parametros nuevos que metemos son:
#   - max_features (el maximo numero de variables que miramos cada vez que dividimo un nodo)
#   - max_values (el maximo numero de valores de cada variable cuantitativa que miramos en cada division)
#   - valores_color (las distintas categorias de la variable categorica color)
#   - valores_... (en caso de que hubiese mas variables categoricas tambien hay que meterals como parametro)
#                  ESTO ESTA UN POCO MAL ASI, PENSAR OTRA FORMA DE PONERLO 
def best_split_gini_random(data,valores_y,partition_list,nodo,numeric_list,iteration,p,max_features,max_values,n_variablesX):
    
    # aqui guardamos la mayor ganancia de infomracion
    best_gain = sfix.Array(1)  
    #Lo inicializamos en 0
    best_gain[0]=sfix(0)

    # aqui guardamos el valor y la columna que hacen la mejor pregunta
    best_question=sfix.Array(2)
    best_question[0]=sfix(0)
    best_question[1]=sfix(0)
    #Por el momento lo inicializamos en [0,0], aunque eso es una pregunta de por si.
    #(no influye, si es la mejor esta bien y si no lo es se reemplazara)

    #start_timer(2)      
    current_uncertainty = gini(data,valores_y,partition_list,nodo)
    #stop_timer(2)

    #Con max_features tenemos el numero de variables que mirar en cada iteracion.
    #Ahora se elige de forma random cuales van a ser esas dos variables

    #Del 0 a n_variablesX elegimos max_features numeros de forma random y los guardamos en una lista.
    #Para que funcione random vamos a tener que guardarlos en una lista de PYTHON
    import random
    random.seed(0)
    variables_division = [random.randint(0,n_variablesX-1) for i in range(max_features)]

    #Definimos un Array para guardar la nueva partition:
    partition_nueva=sfix.Array(len(data))

    

    for i in range(max_features):
        col=variables_division[i]
        print('una de las variables elegidas es: %s',col)
        #Para cada variable explicativa x que se elige de forma random 

        # OPCIONES:
        # 1- Vamos mirando uno a uno cada una de estas filas y debemos comprobar que realmente esten en nuestro nodo
        #Osea que en realidad se miraran incluso menos valores que max_values (porque muchos no estaran en el nodo)
        #No se si es buena idea hacerlo asi, se van a mirar muy pocos valores al final !
        #   Si ponemos un valor muy grande de max_values en la primera iteracion se miraran muchos
        #   pero en las siguientes opraciones pocos
        #   Si ponemos un valor muy peque de max_values en las primeras iteraciones bien, pero en las siguientes apenas 
        #   se miraran valores  

        # 2- Otra forma, dejarlo igual pero elegir de forma random cada valor con 0 o 1 si lo elegimos
        #Ir contando con cuantos nos sale un uno y parar cuando llegamos a max_values
        #Lo malo es que probablemente acabe elgiendo los primeros mas si hay muchos datos y no llegaria hasta los ultimos
        #Otra opcion es elegir de forma random por que indice empezar a mirar y despues hacer eso.
        #ESTO ULTIMO TAMPOCO ES MALA OPCION

        
        # 3- Ir mirando en orden todas las lineas e ir sumando cuantos metemos hasta max_values
        #pero entonces no es random! siempre se mirarian los primeros!!!!!

        # 4- Poner un valor random grande y hacer un while hasta que se acabe la lista o hasta que sea al menos max_values
        
        # 5- DEFINITIVA POR EL MOMENTO Reasignar a cada fila un valor random y hacer un if de que la cuenta de los que se van mirando sea menor que  max_values

        #Elegimos len(data)[Es por asegurarnos de poner un valor grande] indices random donde estaran los valores que van de 0 a numero de lineas del dataset
        random.seed(0)
        n=len(data)
        valores_variables = [random.randint(0,n-1) for j in range(n)]

        #Lo malo es que algunos pueden estar repetidos 

        #Vamos contando el numero de valores que elegimos
        no_values=MemFix(sfix(0))

        for j in range(len(data)):
            #miramos solo hasta max_values valores random dentro de los que toma esa variable 
            @if_e((no_values.read()<max_values).reveal())
            def block(): 
                print_ln('Todavia no he llegado al maximo numero de valores (esto saldra 10 veces)')

                indice_random=valores_variables[j]
                print_ln('El indice random es: %s',indice_random)

                #Miramos los que en la partition list tengan justo ese numero de nodo
                #Es decir,los que realmente pertenecen al data del nodo que queremos dividir        
                @if_e((partition_list[indice_random]==nodo).reveal())
                def block():
                    no_values.iadd(1)
                    print_ln('Este indice esta en el nodo que queremos dividir')

                    valor=data[indice_random][col] #ESTO CAMBIAAAAAAAAAAAAAAAAAAAAAA
                    #para cada valor que puede tomar se realiza la pregunta
                    pregunta = question(col, valor)

                    # Intentar dividir el dataset mediante esa pregunta
                    
                    #start_timer(3) 
                    partition_nueva = partition(data, partition_list,pregunta,numeric_list,nodo,iteration,p)
                    #stop_timer(3)
                    
                    exp2_iter=mpc_math.exp2_fx(iteration)

                    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE FALSE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
                    es_false=nodo + exp2_iter + p

                    #SACAR EL NUMERO QUE CORRESPONDE A LOS DE TRUE DEPENDIENDO DE LA ITERACION Y DE LA RAMA:
                    es_true=nodo + exp2_iter + p + sfix(1)

                    # Si el corte no divide el dataset saltarselo. Para ello, miramos que los numeros
                    # esten al menos una vez en partition_list. Si lo divide estaran los dos numeros,
                    # por lo que con mirar que esta uno de los dos vale, miramos por ejemplo que este es_false
          
                    @if_e((element_search(es_false,partition_nueva)==0).reveal())
                    def block():
                        #Si no esta es porque no lo ha dividio, no actualizamos
                        best_gain[0]=best_gain[0]
                    @else_
                    def block():    
                        # Calcular la ganancia de informacion con ese corte
                        #start_timer(4)
                        gain = info_gain(data,valores_y,partition_nueva, current_uncertainty,nodo,iteration,p)
                        #stop_timer(4)
                                     

                        @if_e((gain>=best_gain[0]).reveal()==1)
                        def block():
                            best_gain[0]=gain
                            best_question[0]=pregunta[0]
                            best_question[1]=pregunta[1]
                        @else_
                        def block():
                            best_gain[0]=best_gain[0]
                @else_
                def block():
                    print_ln('Este indice no esta en el nodo que queremos dividir')
                    #Los que no aparecen en el data que queremos dividir no hay que mirarlos 
                    best_gain[0]=best_gain[0]

            @else_
            def block():
                nada=0
                #Lo ideal seria que saliese del for
                #una especie de break

    return best_gain[0], best_question



#------------------------------------------ FUNCION build_tree ----------------------------------------- 
# Primero se intenta dividir el dataset por cada uno de sus atributos unicos y 
#calcular la ganancia de informacion y la pregunta que lo produce

#Caso base: no hay mas ganancia de informacion, porque no se pueden realizar mas preguntas
#se devuelve el leaf 

#Si no entramos en el caso base es porque se ha encntrado una particion util


#Si todas las variables son cualitativas si que es facil saber cual es el maximo numero de preguntas
#Si hay variables cuantitativas el numero de preguntas no es fijo, porque se pueden ir haciendo cortes cada vez mas fijandonos

#En general como maximo se pueden hacer no de filas -1 preguntas: q preguntas
#Se puede crear una matriz de dimension qx2, para ir guardando las preguntas
#Mirar cuanto se puede reducir el dimension de esta matriz


def build_tree(data,valores_y,numeric_list):

    q=len(data)-1
    preguntas=sfix.Matrix(q,2)

    #Guardamos en una variable el numero de preguntas que se hacen,
    #asi unicamente se leera de la lista preguntas el numero de preguntas que realmente se han realizado
    num_preguntas=MemFix(sfix(0))

    iteration= MemFix(sfix(0))

    #Al principio p es cero
    p=MemFix(sfix(0))


    #partition_list en un principio sera una lista llena de 0s, porque al principio todas las filas  pertenecen al nodo 0   
    partition_list=sfix.Array(len(training_data))
    @for_range(len(training_data))
    def range_body(i):
        partition_list[i]=sfix(0)

    #El nodo empieza en cero y se van mirando todos los nodos
    nodo=MemFix(sfix(0))

    #Con esto indicamos si en esta interacion se ha realizado alguna particion o no
    #En caso de que no se haya relizado ninguna no se debe seguir con la siguiente iteracion
    #Sirve como criterio de parada 
    hace_particion=MemFix(sfix(0))

    gain, question = best_split_gini(data,valores_y,partition_list,nodo,numeric_list,iteration,p)

    #Para ir guaradando las preguntas vamos actualizando un indice:
    indice=sfix.Array(1)
    indice[0]=sfix(0)

    #Definimos un Array para guardar la nueva partition:
    partition_nueva=sfix.Array(len(data))

    #Si la ganancia es distinta de cero se guarda la pregunta y se realiza la hace_particion
    #Al guardar la nueva pregunta se actualiza el indice y se indica que se ha realizado una particion para que entre en el while
    @if_e((gain!=0).reveal()==1)
    def block():
        #indice[0] es un sfix por lo que no se puede meter como indice de preguntas:
        #preguntas[indice[0]][0]
        #muy poco eficiente 
        @for_range(len(preguntas))
        def range_body(k):
            @if_e((indice[0]==k).reveal()==1)
            def body():
                preguntas[k][0]=question[0]
                preguntas[k][1]=question[1]
                num_preguntas.iadd(1)
            @else_
            def body():
                #no quiero que haga nada 
                nada=0
        indice[0]=indice[0]+1
    
        partition_nueva=partition(data,partition_list, question,numeric_list,nodo,iteration,p)
        hace_particion.iadd(1)
        partition_list[:]=partition_nueva
    @else_
    def block():
        partition_list[:]=partition_list

    #Ahora hay que separar en 2^iteration 
    iteration.iadd(1)

    
    @do_while
    def f():

        #Para comprobar que en esta iteracion se ha hecho alguna particion
        #en caso contrario parar el while
        hace_particion.write(sfix(0))

        #Al pricipio de cada iteracion p vuelve a ser cero y por cada nodo que avanza se suma uno:
        p.write(sfix(0))

        #CAMBIAR ESTO Y PONERLO MEJOR 
        @for_range(50)#como maximo va a funcionar para 2^iter en [0,50] nodos en cada iteracion
        def range_body(r):
            @if_e((mpc_math.exp2_fx(iteration)==r).reveal()==1)
            def body():
                @for_range(r)
                def range_body(i):
                    nodo.iadd(1)
                    #Si el nodo madre no esta en partition_list, obviamente no hay que partirlo
                    esta_nodo=element_search(nodo,partition_list)
                    @if_e((esta_nodo.reveal()==1))
                    def block():
                        gain, question = best_split_gini(data,valores_y,partition_list,nodo,numeric_list,iteration,p)
                        @if_e((gain!=0).reveal()==1)
                        def block():
                            #indice[0] es un sfix por lo que no se puede meter como indice de preguntas
                            @for_range(len(preguntas))
                            def range_body(k):
                                @if_e((indice[0]==k).reveal()==1)
                                def body():
                                    preguntas[k][0]=question[0]
                                    preguntas[k][1]=question[1]
                                    num_preguntas.iadd(1)
                                @else_
                                def body():
                                    nada=0
                        
                            indice[0]=indice[0]+1
                            partition_nueva=partition(data,partition_list, question,numeric_list,nodo,iteration,p)
                            hace_particion.iadd(1)
                            partition_list[:]=partition_nueva
                        @else_
                        def block():
                            partition_list[:]=partition_list
                    @else_
                    def block():
                        partition_list[:]=partition_list
                    #Despues de cada nodo a p se le suma uno:
                    p.iadd(1)
            @else_
            def block():
                p.iadd(0)
            
        iteration.iadd(1)

        
        return hace_particion.read().reveal()!=0
        
        #return and_(lambda:iteration<=max_iteration, lambda:hace_particion.read()!=0)

    #Aparte de la iteracion, tambien se tiene que parar cuando ya no hay mas nodos madres que mirar, es decir
    #si por ejemplo en la iteracion 2 se intentan dividir los nodos 5 y 6 pero no aumenta el gain
    #no tiene sentido hacer la iteracion 3. Algo que indique que la partition_list ha cambiado respecto a la iteracion anterior

    #El loop para cuando el return value es cero o false
    #Aqui tambien pasa que las local variables no pueden ser modificdas y hay que 
    #utilizar memvalues 


    return preguntas,num_preguntas,partition_list
    
#----------------------------------- Comprobacion build_tree ----------------------------------------

preguntas,numero_preguntas,partition_list=build_tree(training_data,valores_y,numeric_list)

print_ln('\n RESULTADO DEL ARBOL DE DECISION: \n')

numero=numero_preguntas.read()

print_ln('Numero de preguntas es: %s',numero.reveal())

@for_range(len(preguntas))
def range_body(i):
    @if_e((numero<=i).reveal()==1)
    def block():
        nada=0
    @else_
    def block():
        print_ln('\nLa pregunta %s es:',i)
        @for_range(2)
        def reange_body(j):
            print_str('%s ', preguntas[i][j].reveal()) #Asi se imprime toda la pregunta en una linea

print_ln('\n La lista indicando cada fila a que nodo pertenece es: \n')
@for_range(len(training_data))
def range_body(i):
    print_str('%s ', partition_list[i].reveal()) #Asi se imprime toda la lista en una linea

print_ln('\n')

#....FUNCIONA CORRECTAMENTE...


